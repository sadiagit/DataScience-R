---
title: "Ensembles"
author: "Sadia Boksh"
date: "19/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For these exercises we are going to build several machine learning models for the mnist_27 dataset and then build an ensemble. Each of the exercises in this comprehension check builds on the last.
Use the training set to build a model with several of the models available from the caret package. We will test out 10 of the most common machine learning models in this exercise:

```{r warning=FALSE}
models <- c("glm", "lda", "naive_bayes", "svmLinear", "knn", "gamLoess", "multinom", "qda", "rf", "adaboost")
library(caret)
library(dslabs)
library(tidyverse)
# set.seed(1) # if using R 3.5 or earlier
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
data("mnist_27")

fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 

names(fits) <- models
```

Now that you have all the trained models in a list, use sapply() or map() to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models) columns.

What are the dimensions of the matrix of predictions?

```{r}

preds <- sapply(fits, function(m){
  predict(m, mnist_27$test)
})
dim(preds)
```

Now compute accuracy for each model on the test set.

Report the mean accuracy across all models.

```{r}
acc_per_method <- colMeans(preds==mnist_27$test$y)
mean(acc_per_method)

```

Next, build an ensemble prediction by majority vote and compute the accuracy of the ensemble. Vote 7 if more than 50% of the models are predicting a 7, and 2 otherwise.

What is the accuracy of the ensemble?

```{r}

ensm_by_majority<- ifelse(rowMeans(preds == 7) >0.5, 7, 2)
acc_ensm_by_majority<- mean(ensm_by_majority == mnist_27$test$y)


```

In Q3, we computed the accuracy of each method on the test set and noticed that the individual accuracies varied.

How many of the individual methods do better than the ensemble?

```{r}

sum((acc_per_method > acc_ensm_by_majority))


```

It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the minimum accuracy estimates obtained from cross validation with the training data for each model from fit$results$Accuracy. Obtain these estimates and save them in an object. Report the mean of these training set accuracy estimates.

What is the mean of these training set accuracy estimates?

```{r}
  training_acc <- sapply(fits, function(fit){
    
    min(fit$results$Accuracy)
})
mean(training_acc)

```

Now let's only consider the methods with a minimum accuracy estimate of greater than or equal to 0.8 when constructing the ensemble. Vote 7 if 50% or more of those models are predicting a 7, and 2 otherwise.

What is the accuracy of the ensemble now?

```{r}
models <-  training_acc >=0.8
preds[,models]
ensm_by_min_train_acc<- ifelse( rowMeans(preds[,models] == 7)>0.5, 7, 2)
acc_ensm_by_train_acc<- mean(ensm_by_min_train_acc == mnist_27$test$y)

```
