library(dplyr)
# set.seed(1) # if using R 3.5 or earlier


set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
n <- c(100, 500,1000, 5000, 10000)
rmses <- map(n, function(n_obs){

  #build the function
  Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
  dat <- MASS::mvrnorm(n = n_obs, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))

  #calc rmses for 100 linear models
  rmses <- replicate(100, {
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)

  fit <- train_set %>% lm(y~x, data=.)
  y_hat <- predict(fit,test_set)
  sqrt(mean((y_hat -  test_set$y)^2))

})
  list(m=mean(rmses),sd=sd(rmses))
 # n_obs
  })

df <- data.frame(matrix(unlist(rmses), nrow=length(rmses), byrow=T))

df %>% ggplot(aes(x=n,y=X1))+geom_point()
df %>% ggplot(aes(x=n,y=X2))+geom_point()



#correlation X and Y is larger

# set.seed(1) # if using R 3.5 or earlier
 # if using R 3.6 or later
set.seed(1, sample.kind="Rounding")
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
  data.frame() %>% setNames(c("x", "y"))
set.seed(1, sample.kind="Rounding")
rmses <- replicate(100, {
    test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

    train_set <- dat %>% slice(-test_index)
    test_set <- dat %>% slice(test_index)

    fit <- train_set %>% lm(y~x, data=.)
    y_hat <- predict(fit,test_set)
    sqrt(mean((y_hat -  test_set$y)^2))

  })
  mean(rmses)
  sd(rmses)

  #When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y.




  #multiple predictors

  # set.seed(1) # if using R 3.5 or earlier
  set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
  Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
  dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
    data.frame() %>% setNames(c("y", "x_1", "x_2"))
  cor(dat)

  set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)

  fit_x1 <- train_set %>% lm(y~x_1, data=.)
  y_hat_x1 <- predict(fit_x1,test_set)
  rmse_x1<- sqrt(mean((y_hat_x1 -  test_set$y)^2))


  fit_x2 <- train_set %>% lm(y~x_2, data=.)
  y_hat_x2 <- predict(fit_x2,test_set)
  rmse_x2<- sqrt(mean((y_hat_x2 -  test_set$y)^2))


  fit_x12 <- train_set %>% lm(y~x_1+x_2, data=.)
  y_hat_x12 <- predict(fit_x12,test_set)
  rmse_x12<- sqrt(mean((y_hat_x12 -  test_set$y)^2))


  #high correlation between x_1 nad x_2

  # set.seed(1) # if using R 3.5 or earlier
  set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
  Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
  dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
    data.frame() %>% setNames(c("y", "x_1", "x_2"))


  cor(dat)

  set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)

  train_set <- dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)

  fit_x1 <- train_set %>% lm(y~x_1, data=.)
  y_hat_x1 <- predict(fit_x1,test_set)
  sqrt(mean((y_hat_x1 -  test_set$y)^2))


  fit_x2 <- train_set %>% lm(y~x_2, data=.)
  y_hat_x2 <- predict(fit_x2,test_set)
  sqrt(mean((y_hat_x2 -  test_set$y)^2))


  fit_x12 <- train_set %>% lm(y~x_1+x_2, data=.)
  y_hat_x12 <- predict(fit_x12,test_set)
  sqrt(mean((y_hat_x12 -  test_set$y)^2))
  
  
  # Logistic Regression
```{r}
  
# set.seed(2) #if you are using R 3.5 or earlier
set.seed(2, sample.kind="Rounding") #if you are using R 3.6 or later
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),
	test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()


dat$train %>% ggplot(aes(x, color = y)) + geom_density()

set.seed(1, sample.kind = "Rounding")
mu_1 <- seq(0, 3, len=25)
acc_delta <- map(mu_1, function(delta){

    dat <- make_data(mu_1 = delta)
    fit_glm  <- dat$train %>% glm(y ~ x, data=., family ="binomial")
    p_hat_logit<-predict.glm(fit_glm, newdata = dat$test, )
    y_hat_logit <- ifelse(p_hat_logit >0.5, 1, 0) %>% factor
    accuracy <-confusionMatrix(y_hat_logit, dat$test$y)$overall["Accuracy"]
    list(x=delta,y=accuracy)
  
})
df <- data.frame(matrix(unlist(acc_delta), nrow=length(acc_delta), byrow=T))
df %>% ggplot(aes(x=X1,y=X2)) + geom_point()
```
  

