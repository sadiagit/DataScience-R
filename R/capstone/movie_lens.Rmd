---
title: "Movie Recommendation System"
author: "Sadia Boksh"
date: "29/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

For this project, I will be creating a movie recommendation system using the 10M verson of the MovieLens data set. Here I will train a machine learning model using the inputs in one subset to predict movie ratings in the validation set.

Multiple models to be fit for this purpose and their RMSEs will be calculated. I will start with a simple model that uses the movie average for prediction. Eventually, I will modify the model to include movie effects, user effects and genre effects. In addition, I will use regularization to add penalty terms to shrink the effect of smaller sample sizes towards 0. The model that produces the least RMSE will be used on the validation data set to calculate the final RMSE.

# Analysis

First I will do some basic data exploration. The MovieLens data set will be initially split into 10% validation set for calculating final RMSE and 90% training set to train and test my models and will be called edx set. 


```{r include=FALSE}
#Create train and validation sets
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(stringr)
library(matrixStats)
library(tidyr)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip


#if(!file.exists("ml-10M100K")){
  dl <- tempfile()

  download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
#}

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

Number of rows and cols in the edx set:

```{r echo=FALSE}

#number of rows and cols in edx
nrow(edx)
ncol(edx)
```

Number of rows and cols in the validation set:

```{r echo=FALSE}

#number of rows and cols in edx
nrow(validation)
ncol(validation)
```

Number of zeros as ratings:

```{r echo=FALSE}

edx %>% filter(rating == 0) %>% dplyr::count()
```

Number of 3's as rating: 

```{r echo=FALSE}
edx %>% filter(rating == 3) %>% dplyr::count()

```

Number of different movies:

```{r echo=FALSE}
length(unique(edx$movieId))
```

Number of different user:

```{r echo=FALSE}
length(unique(edx$userId))
```

Number movie ratings in Drama, Comedy, Thriller, Romance in edx dataset:

```{r echo=FALSE,warning=FALSE,message=FALSE}
#filter by drama
drama <- edx %>% filter(str_detect(genres, pattern = "Drama")) 

#filter by comedy
comedy <- edx %>% filter(str_detect(genres, pattern = "Comedy")) 

#filter by thriller
thriller <- edx %>% filter(str_detect(genres, pattern = "Thriller")) 

#filter by romance
romance <- edx %>% filter(str_detect(genres, pattern = "Romance")) 

#create a data frame to show number of ratings in each genre filtered above
data.frame("Genre" = c("Drama", "Comedy", "Thriller", "Romance"), "n" = c(nrow(drama),nrow(comedy),nrow(thriller),nrow(romance)))
```

Highest rated movies:

```{r echo=FALSE, warning=FALSE, message=FALSE}
#group by movieId to summarize highest rated movie
edx %>% group_by(movieId) %>% summarise(n=n(), title= title[1]) %>% arrange(desc(n)) 
```

Greatest number of ratings:

``` {r echo=FALSE,warning=FALSE,message=FALSE}
#group by rating to summarize which rating are rated more
edx %>% group_by(rating) %>% summarise(n=n()) %>% arrange(desc(n)) %>%ggplot(aes(x = rating, y = n)) +
	geom_line()

```

From the plot above we can say that the half star ratings are less common than full star ratings.

## Model Fitting

In order to train and test my model, I will further split up the edx dataset into 20% test set and 80% train set.

```{r echo=FALSE, warning=FALSE, message=FALSE}

set.seed(755, sample.kind="Rounding") 
test_indx <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_indx,]
temp_test_set <- edx[test_indx,]


# ensure userId and movieId in test set are also in train set
test_set <- temp_test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

removed_test <- anti_join(temp_test_set, test_set)

# add back the removed rows from temp to train set
train_set <- rbind(train_set, removed_test)

```


Number of rows in test and train set respectively:
```{r  echo=FALSE, warning=FALSE, message=FALSE}
nrow(test_set)

nrow(train_set)
```

### First Model

For the first model, we will use just the average of all movies for prediction. RMSE for this simple model in the test set:

```{r  echo=FALSE,warning=FALSE,message=FALSE}
#using movie avg to predict rating
mu_hat <- mean(train_set$rating)
just_avg <- RMSE(mu_hat, test_set$rating)
just_avg
```

### Movie Effect

Next I will cater for the movie effect. For movie effect my model is:

$Y_{u,i}$ = $\mu$ + $b_{i}$ + $epsilon _{i}$

and RMSE for this model:

```{r echo=FALSE,warning=FALSE,message=FALSE}

mu <-  mean(train_set$rating)

#group by movieId to find movie effect
movie_avgs <- train_set %>%
              group_by(movieId) %>% 
              summarise(b_i =  mean(rating - mu))

#on the test set, calculate rating using movie effect
pred_with_movie_effect <-  test_set %>% 
                            left_join(movie_avgs,by='movieId') %>%
                            mutate(pred = mu + b_i)%>%
                            pull(pred)


#RMSE when bias
movie_effect <- RMSE(pred_with_movie_effect, test_set$rating)
movie_effect

```

We can also visualize the movie effect:

```{r echo=FALSE,warning=FALSE,message=FALSE}
movie_avgs %>% ggplot(aes(b_i))+geom_histogram(bins=10, color=I("black"))
```

### User Effect

I will also add the user effect to my model. For user effect my model is:

$Y_{u,i}$ = $\mu$ + $b_{i}$ + $b_{u}$ + $epsilon _{i}$


RMSE for this model:

```{r echo=FALSE,warning=FALSE,message=FALSE}
mu <-  mean(train_set$rating)

#group by user id and calculate user effect b_u

user_avgs <- train_set %>% left_join(movie_avgs, by='movieId') %>% group_by(userId) %>% summarise(b_u =  mean(rating - mu -b_i))

#on the test set, calculate rating using user effect
pred_with_user_effect <-  test_set %>%
          left_join(movie_avgs,by='movieId') %>%
          left_join(user_avgs,by='userId') %>% 
          mutate(pred = mu + b_i + b_u) %>% pull(pred)


#RMSE when bias
user_effect <-RMSE(pred_with_user_effect, test_set$rating)
user_effect
```

We can also check visually how user to user variability looks like:

```{r echo=FALSE,warning=FALSE, message=FALSE}

user_avgs %>% ggplot(aes(b_u))+geom_histogram(color=I("black"))
```

### Genre Effect

Next I would like to see how genres take part in movie rating. We can visualize genre to genre variability in rating:

```{r echo=FALSE,warning=FALSE, message=FALSE}
 
train_set %>% group_by(genres) %>%
              summarize(n_of_ratings =  n()) %>%
              filter(n_of_ratings > 50000) %>%
              left_join(train_set , by="genres") %>%
              group_by(genres) %>%
              summarise(n = n(),avg_rating = mean(rating), se=sd(rating)/sqrt(n)) %>% 
              ggplot(aes(x = genres, y = avg_rating)) +
              geom_errorbar(aes(ymin = avg_rating-2*se, ymax = avg_rating+2*se))+theme(axis.text  = element_text(angle = 90))

```

As from the plot above we can see, a movie can belong to one or more genres, we need to add up all the genre effects that the movie belongs to . For the genres effect my model is:

$Y_{u,i}$ = $\mu$ + $b_{i}$ + $b_{u}$ + $\sum_{k=1}^{K} X_{u,i}b_{k}$ with $x_{u,i}^k$ = 1 if $g_{u,i}$ is genre k.

RMSE for this model:

```{r echo=FALSE,warning=FALSE,message=FALSE}
mu <-  mean(train_set$rating)
#ensure genres in test set are also in train set
test_genres <- semi_join(test_set,train_set, by="genres")

# group by genres to calculate genres effect
genre_avgs <- train_set %>% 
                         left_join(movie_avgs, by='movieId') %>%
                         left_join(user_avgs, by='userId') %>%
                         group_by(genres) %>% 
                         summarise(b_g = mean(rating - mu - b_i - b_u))
#on the test set, calculate rating using genre effect
pred_with_genre_effect <-  test_genres %>% 
                
                left_join(movie_avgs,by='movieId') %>%
                left_join(user_avgs,by='userId') %>%
                left_join(genre_avgs, by="genres") %>%
                mutate(pred = mu + b_i + b_u + b_g) %>%
                pull(pred)



#RMSE when combined genre bias# 
genre_effect <-RMSE(pred_with_genre_effect, test_set$rating)

genre_effect
```

```{r echo=FALSE,warning=FALSE,message=FALSE}

# #Separate train and test set rows for each genre each movie belongs
# train_genres <- train_set %>% separate_rows(genres) %>% filter(genres != '') 
# test_genres <- test_set %>% separate_rows(genres) %>% filter(genres != '') 
# 
# #ensure genres in test set are also in train set
# test_genres <- semi_join(test_genres,train_genres, by="genres")
# 
# # group by genres to calculate genres effect
# genre_avgs <- train_genres %>% filter(genres != '') %>%
#                          left_join(movie_avgs, by='movieId') %>%
#                          left_join(user_avgs, by='userId') %>%
#                          group_by(genres) %>% 
#                          summarise(b_g = mean(rating - mu_hat - b_i - b_u))
# # each movie belongs to one or multiple genres, we need to sum up all the genres effect a movie belongs to
# bg_combined <-  test_genres %>% 
#                 filter(genres != '') %>%
#                 left_join(movie_avgs,by='movieId') %>%
#                 left_join(user_avgs,by='userId') %>%
#                 left_join(genre_avgs, by="genres") %>%
#                 group_by(movieId,userId) %>% 
#                 summarise(b_g_combined = sum(b_g)) %>%
#                 pull(b_g_combined)
# 
# 
# # calculate rating with genre effect
# pred_with_genre_effect <-  mu_hat + bi + bu + bg_combined
# # 
# # #RMSE when combined genre bias# 
# genre_effect <-RMSE(pred_with_genre_effect, test_set$rating)
# 
# genre_effect
```




### Regularization

Finally I will use regularization to penalize the genre, user and movie bias and shrink them towards 0 when sample sizes are small.
I will use tuning parameter lambda from 3 to 6.

```{r echo=FALSE, message=FALSE, warning=FALSE}
lambdas <- seq(3, 6, 0.25)

rmses <- sapply(lambdas, function(l){
    mu <-  mean(train_set$rating)
    movie_reg_avgs <- train_set %>%
                      group_by(movieId) %>%
                      summarize(b_i = sum(rating - mu)/(n()+l))

    user_reg_avgs <- train_set %>% left_join(movie_reg_avgs, by="movieId") %>%
                      group_by(userId) %>%
                      summarize(b_u = sum(rating - mu - b_i)/(n()+l))
    
    genre_reg_avgs <- train_set %>% 
                         left_join(movie_reg_avgs, by='movieId') %>%
                         left_join(user_reg_avgs, by='userId') %>%
                         group_by(genres) %>% 
                         summarise(b_g = sum(rating - mu - b_i - b_u)/(n()+l))
    
    predicted_ratings <- test_set %>%
                                left_join(movie_reg_avgs, by='movieId') %>%
                                left_join(user_reg_avgs, by='userId') %>%
                                left_join(genre_reg_avgs, by='genres') %>%
                                mutate(pred = mu + b_i + b_u+b_g) %>%
                                pull(pred)
    
   RMSE(predicted_ratings, test_set$rating)
})
qplot(lambdas, rmses)
```


lambda that minimizes RMSE:

```{r echo=FALSE, message=FALSE, warning=FALSE }
lambdas[which.min(rmses)]

```

RMSE for regularized  movie, user and genre effect on the test set:

```{r echo=FALSE,warning=FALSE,message=FALSE}
reg_movie_user_genre_effect <- min(rmses)
reg_movie_user_genre_effect
```


# Result

From the analysis above we can see that the RMSE keeps getting better as we include user effect, movie effect and genre effect in the model and regularization makes it perform even better. Below is the table that shows RMSEs achieved in different models when testing on the test set (20% of edx set)

```{r echo=FALSE,warning=FALSE,message=FALSE}
data.frame("Method"=c('Just Avg', 'Movie Effect', 'User Effect', 'Genre Effect', 'Regularized Movie + User + Genres Effect'), "RMSE"= c(just_avg, movie_effect, user_effect, genre_effect,reg_movie_user_genre_effect ))

```

We can see regularized model with movie, user and genre effect gives us the best RMSE on the test set (20% of edx set), so this is our preferred model.

I will apply the regularized model on the validation set to calculate the final RMSE.

### Final RMSE

Final RMSE will be calculated on the validation set. I will use the lambda = 4.75 on the validation set.

Final RMSE on validation set:

```{r echo=FALSE,warning=FALSE,message=FALSE}

lambda <- 4.75
mu_edx <-  mean(edx$rating)
movie_reg_avgs <- edx %>%
                  group_by(movieId) %>%
                  summarize(b_i = sum(rating - mu_edx)/(n()+lambda))

user_reg_avgs <- edx %>% left_join(movie_reg_avgs, by="movieId") %>%
                  group_by(userId) %>%
                  summarize(b_u = sum(rating - mu_edx - b_i)/(n()+lambda))

genre_reg_avgs <-    edx %>% 
                     left_join(movie_reg_avgs, by='movieId') %>%
                     left_join(user_reg_avgs, by='userId') %>%
                     group_by(genres) %>% 
                     summarise(b_g = sum(rating - mu_edx - b_i - b_u)/(n()+lambda))

predicted_ratings <- validation %>%
                            left_join(movie_reg_avgs, by='movieId') %>%
                            left_join(user_reg_avgs, by='userId') %>%
                            left_join(genre_reg_avgs, by='genres') %>%
                            mutate(pred = mu_edx + b_i + b_u+b_g) %>%
                            pull(pred)
RMSE(predicted_ratings, validation$rating)

```


# Conclusion

In summary, regularization has improved the prediction algorithm and produced the best RMSE (0.8644514) on the validation set. One limitation of this model is that it does not cater for user movie preferences or movie rating pattern. Future work can be done in this area by doing factor analysis using Matrix factorization. 
